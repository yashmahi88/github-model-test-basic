name: Update Dynamic Knowledge Base

on:
  workflow_dispatch:

jobs:
  collect-logs:
    runs-on: ubuntu-latest
    env:
      GH_TOKEN: ${{ secrets.PERSONAL_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Fetch and Segregate GitHub Actions Logs
        run: |
          mkdir -p temp_logs/nodejs temp_logs/python temp_logs/unclassified # Create unclassified directory

          echo "Node.js GitHub Logs - $(date)" > temp_logs/nodejs/nodejs_combined_logs.txt
          echo "Python GitHub Logs - $(date)" > temp_logs/python/python_combined_logs.txt
          echo "Unclassified GitHub Logs - $(date)" > temp_logs/unclassified/unclassified_combined_logs.txt # Header for unclassified

          # IMPORTANT: Use the exact display name of your workflow, not just the file name.
          # Based on previous conversation, the workflow display name is "Azure AI Foundry Prediction"
          # If "python-ci-prediction.yml" is the new display name, use that.
          gh run list --workflow="python-ci-prediction.yml" --limit 20 \
            --json databaseId,displayTitle,headBranch,conclusion,status,url,startedAt,updatedAt > temp_logs/run_history.json

          # Process each completed run and segregate by language
          jq -r '.[] | select(.status == "completed") | .databaseId' temp_logs/run_history.json | while read -r RUN_ID; do
            echo "Processing run $RUN_ID..."
            
            # Get the full log for this run
            gh run view "$RUN_ID" --log > temp_logs/temp_run_log.txt
            
            # --- DEBUGGING AID (UNCOMMENT TO SEE LOG CONTENT) ---
            # echo "--- START LOG CONTENT for RUN $RUN_ID ---"
            # cat temp_logs/temp_run_log.txt
            # echo "--- END LOG CONTENT for RUN $RUN_ID ---"
            # ----------------------------------------------------

            # Define robust patterns for classification (from previous solution)
            # Make sure these strings actually appear in your workflow's logs for each type.
            NODE_PATTERN="LANGUAGE=nodejs|main-ci.yml|Setting Node.js environment variables"
            PYTHON_PATTERN="LANGUAGE=python|python-ci.yml|Setting Python environment variables"

            
            IS_NODE=false
            IS_PYTHON=false

            # Check for Node.js indicators
            if grep -qE "$NODE_PATTERN" temp_logs/temp_run_log.txt; then
              IS_NODE=true
            fi
            
            # Check for Python indicators
            if grep -qE "$PYTHON_PATTERN" temp_logs/temp_run_log.txt; then
              IS_PYTHON=true
            fi

            # Now, based on independent checks, decide where to put the log
            if $IS_NODE && ! $IS_PYTHON; then
              # This run is clearly Node.js
              echo -e "\n========== NODE.JS RUN $RUN_ID ==========\n" >> temp_logs/nodejs/nodejs_combined_logs.txt
              cat temp_logs/temp_run_log.txt >> temp_logs/nodejs/nodejs_combined_logs.txt
              echo "✅ Run $RUN_ID classified as Node.js"
            elif $IS_PYTHON && ! $IS_NODE; then
              # This run is clearly Python
              echo -e "\n========== PYTHON RUN $RUN_ID ==========\n" >> temp_logs/python/python_combined_logs.txt
              cat temp_logs/temp_run_log.txt >> temp_logs/python/python_combined_logs.txt
              echo "✅ Run $RUN_ID classified as Python"
            elif $IS_NODE && $IS_PYTHON; then
              # This run matches BOTH patterns - this is a conflict, handle as unclassified or prioritize
              # For now, put in unclassified for debugging, but you could prioritize one or append to both.
              echo -e "\n========== CONFLICT: RUN $RUN_ID (Matched Both Node.js and Python Patterns) ==========\n" >> temp_logs/unclassified/unclassified_combined_logs.txt
              cat temp_logs/temp_run_log.txt >> temp_logs/unclassified/unclassified_combined_logs.txt
              echo "⚠️ Run $RUN_ID matched both Node.js and Python patterns. Added to unclassified."
            else
              # This run matches NEITHER pattern - it's truly unclassified
              echo -e "\n========== UNCLASSIFIED RUN $RUN_ID (No Language Pattern Match) ==========\n" >> temp_logs/unclassified/unclassified_combined_logs.txt
              cat temp_logs/temp_run_log.txt >> temp_logs/unclassified/unclassified_combined_logs.txt
              echo "⚠️ Run $RUN_ID could not be classified. Added to unclassified."
            fi
            
            # Clean up temp file
            rm -f temp_logs/temp_run_log.txt
          done

          # Check what we collected
          echo "Node.js logs size: $(wc -l < temp_logs/nodejs/nodejs_combined_logs.txt) lines"
          echo "Python logs size: $(wc -l < temp_logs/python/python_combined_logs.txt) lines"
          echo "Unclassified logs size: $(wc -l < temp_logs/unclassified/unclassified_combined_logs.txt) lines" # Display unclassified too

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      - name: Upload Node.js Logs to Azure Blob Storage
        run: |
          if [[ -s temp_logs/nodejs/nodejs_combined_logs.txt ]]; then
            echo "Uploading Node.js logs to Azure Storage..."
            az storage blob upload \
              --account-name sa1testeicyashmahin9324 \
              --container-name fileupload-test-data-index \
              --name "nodejs_dynamic_combined_logs_$(date +%Y%m%d_%H%M%S).txt" \
              --file temp_logs/nodejs/nodejs_combined_logs.txt \
              --auth-mode key \
              --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
              --overwrite
            echo "Node.js logs uploaded successfully"
          else
            echo "No Node.js logs to upload"
          fi

      - name: Upload Python Logs to Azure Blob Storage
        run: |
          if [[ -s temp_logs/python/python_combined_logs.txt ]]; then
            echo "Uploading Python logs to Azure Storage..."
            az storage blob upload \
              --account-name sa1testeicyashmahin9324 \
              --container-name python-workflow-data-index \
              --name "python_dynamic_combined_logs_$(date +%Y%m%d_%H%M%S).txt" \
              --file temp_logs/python/python_combined_logs.txt \
              --auth-mode key \
              --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
              --overwrite
            echo "Python logs uploaded successfully"
          else
            echo "No Python logs to upload"
          fi

      - name: Generate Upload Summary
        run: |
          echo "========== UPLOAD SUMMARY =========="
          echo "Timestamp: $(date)"
          
          if [[ -s temp_logs/nodejs/nodejs_combined_logs.txt ]]; then
            NODE_LINES=$(wc -l < temp_logs/nodejs/nodejs_combined_logs.txt)
            echo "Node.js logs: $NODE_LINES lines uploaded to sa1testeicyashmahin9324/fileupload-test-data-index"
          else
            echo "Node.js logs: No logs found"
          fi
          
          if [[ -s temp_logs/python/python_combined_logs.txt ]]; then
            PYTHON_LINES=$(wc -l < temp_logs/python/python_combined_logs.txt)
            echo "Python logs: $PYTHON_LINES lines uploaded to sa1testeicyashmahin9324/python-workflow-data-index"
          else
            echo "Python logs: No logs found"
          fi

      - name: Upload Log Analysis Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: log-segregation-summary-${{ github.run_number }}
          path: |
            temp_logs/run_history.json
            temp_logs/nodejs/nodejs_combined_logs.txt
            temp_logs/python/python_combined_logs.txt

      - name: Clean Up Temporary Files
        run: rm -rf temp_logs




# name: Update Dynamic Knowledge Base

# on:
#   workflow_dispatch:

# jobs:
#   collect-logs:
#     runs-on: ubuntu-latest
#     env:
#       GH_TOKEN: ${{ secrets.PERSONAL_TOKEN }}

#     steps:
#       - name: Checkout repository
#         uses: actions/checkout@v4

#       - name: Fetch and Combine GitHub Actions Logs
#         run: |
#           mkdir -p temp_logs

#           echo "Combined GitHub Logs - $(date)" > temp_logs/dynamic_combined_logs.txt

#           gh run list --workflow="train-precheck.yml" --limit 10 \
#             --json databaseId,displayTitle,headBranch,conclusion,status,url,startedAt,updatedAt > temp_logs/run_history.json

#           jq -r '.[] | select(.status == "completed") | .databaseId' temp_logs/run_history.json | while read -r RUN_ID; do
#             echo -e "\n========== RUN $RUN_ID ==========\n" >> temp_logs/dynamic_combined_logs.txt
#             gh run view "$RUN_ID" --log >> temp_logs/dynamic_combined_logs.txt
#           done

#       - name: Install Azure CLI
#         run: |
#           curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

#       - name: Upload Combined Log File to Azure Blob Storage
#         run: |
#           echo "Uploading to Azure Storage..."
#           az storage blob upload \
#             --account-name ${{ vars.AZURE_STORAGE_NAME }} \
#             --container-name fileupload-test-data-index \
#             --name dynamic_combined_logs.txt \
#             --file temp_logs/dynamic_combined_logs.txt \
#             --auth-mode key \
#             --account-key ${{ secrets.AZURE_STORAGE_KEY }} \
#             --overwrite

#       - name: Clean Up Temporary Files
#         run: rm -rf temp_logs
